[
    {
        "chunk_id": 1,
        "content": "## CHAPTER 1 Hello Transformers\nIn 2017, researchers at Google published a paper that proposed a novel neural network architecture for sequence modeling. 1 Dubbed the Transformer , this architecture outperformed recurrent neural networks (RNNs) on machine translation tasks, both in terms of translation quality and training cost. In parallel, an effective transfer learning method called ULMFiT showed that training long short-term memory (LSTM) networks on a very large and diverse corpus could produce state-of-the-art text classifiers with little labeled data. 2 These advances were the catalysts for two of today's most well-known transformers: the Generative Pretrained Transformer (GPT) 3 and Bidirectional Encoder Representations from Transformers (BERT). 4 By combining the Transformer architecture with unsupervised learning, these models removed the need to train task-specific architectures from scratch and broke almost every benchmark in NLP by a significant margin. Since the release of GPT and BERT, a zoo of transformer models has emerged; a timeline of the most prominent entries is shown in Figure 1-1. 1 A. Vaswani et al., ' Attention Is All Y ou Need', (2017). This title was so catchy that no less than 50 follow-up papers have included 'all you need' in their titles! 2 J. Howard and S. Ruder, 'Universal Language Model Fine-Tuning for Text Classification', (2018). 3 A. Radford et al., 'Improving Language Understanding by Generative Pre-Training', (2018). 4 J. Devlin et al., 'BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding', (2018). Figure 1-1. The transformers timeline",
        "metadata": {
            "source": "part1.md",
            "section": "CHAPTER 1 Hello Transformers",
            "position": 1
        }
    },
    {
        "chunk_id": 2,
        "content": "## CHAPTER 1 Hello Transformers\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000000_ae6aab24c545cf3bbbf12b9dc51f329fbea6535c7652dd12c743bb476db62919.png) *Image Description:* The image is a timeline chart showing the development and progression of transformer-based models over time, from 2017 to 2021. The timeline is marked at intervals of one year, with red vertical lines indicating the introduction of various transformer models. The earliest marked year is 2017, where the Transformer model was initially introduced. Key milestones include: ULMFiT in 2018, GPT in the same year, followed by BERT in 2019, which also sets the stage for GPT-2. The year 2020 sees the introduction of RoBERTa, XLM-R, and DistilBERT, while 2021 marks the release of DeBERTa, GPT-Neo, GPT-3, T5, and GPT-J. This chart illustrates the rapid evolution and widespread adoption of transformer models in natural language processing and related fields. But we're getting ahead of ourselves. To understand what is novel about transformers, we first need to explain: - The encoder-decoder framework - Attention mechanisms - Transfer learning In this chapter we'll introduce the core concepts that underlie the pervasiveness of transformers, take a tour of some of the tasks that they excel at, and conclude with a look at the Hugging Face ecosystem of tools and libraries. Let's start by exploring the encoder-decoder framework and the architectures that preceded the rise of transformers.",
        "metadata": {
            "source": "part1.md",
            "section": "CHAPTER 1 Hello Transformers",
            "position": 2
        }
    },
    {
        "chunk_id": 3,
        "content": "## The Encoder-Decoder Framework\nPrior to transformers, recurrent architectures such as LSTMs were the state of the art in NLP. These architectures contain a feedback loop in the network connections that allows information to propagate from one step to another, making them ideal for modeling sequential data like text. As illustrated on the left side of Figure 1-2, an RNN receives some input (which could be a word or character), feeds it through the network, and outputs a vector called the hidden state . At the same time, the model feeds some information back to itself through the feedback loop, which it can then use in the next step. This can be more clearly seen if we 'unroll' the loop as shown on the right side of Figure 1-2: the RNN passes information about its state at each step to the next operation in the sequence. This allows an RNN to keep track of information from previous steps, and use it for its output predictions. Figure 1-2. Unrolling an RNN in time",
        "metadata": {
            "source": "part1.md",
            "section": "The Encoder-Decoder Framework",
            "position": 3
        }
    },
    {
        "chunk_id": 4,
        "content": "## The Encoder-Decoder Framework\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000001_96a5422c82ff72229c77e8b0d83b1bc4c9783eddce1567fa3e4a049c4693670b.png) *Image Description:* The image depicts the process of unrolling a Recurrent Neural Network (RNN) in time, showing how the network processes a sequence of inputs over time. Each cell in the RNN represents a single time step, where the state of the network at one time step is influenced by the input at that time step and the state from the previous time step. The figure illustrates the sequence of states and inputs, where the RNN cell at each time step receives an input from the input sequence and produces an output and a new state, which is then fed back into the next time step. This process is repeated for each time step in the sequence, effectively unrolling the RNN into a series of interconnected computational steps. These architectures were (and continue to be) widely used for NLP tasks, speech processing, and time series. You can find a wonderful exposition of their capabilities in Andrej Karpathy's blog post, 'The Unreasonable Effectiveness of Recurrent Neural Networks'. One area where RNNs played an important role was in the development of machine translation systems, where the objective is to map a sequence of words in one language to another. This kind of task is usually tackled with an encoder-decoder or sequence-to-sequence architecture, 5 which is well suited for situations where the input and output are both sequences of arbitrary length. The job of the encoder is to encode the information from the input sequence into a numerical representation that is often called the last hidden state . This state is then passed to the decoder, which generates the output sequence. In general, the encoder and decoder components can be any kind of neural network architecture that can model sequences. This is illustrated for a pair of RNNs in Figure 1-3, where the English sentence 'Transformers are great!' is encoded as a hidden state vector that is then decoded to produce the German translation 'Transformer sind grossartig!' The input words are fed sequentially through the encoder and the output words are generated one at a time, from top to bottom. 5 I. Sutskever, O. Vinyals, and Q.V . Le, 'Sequence to Sequence Learning with Neural Networks', (2014). Figure 1-3. An encoder-decoder architecture with a pair of RNNs (in general, there are many more recurrent layers than those shown here) ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000002_23521f3d35422fa73585da0eee8da7a826bc7f4d395547c3f1947cee7c69db61.png) *Image Description:* The image illustrates an encoder-decoder architecture utilizing recurrent neural networks",
        "metadata": {
            "source": "part1.md",
            "section": "The Encoder-Decoder Framework",
            "position": 4
        }
    },
    {
        "chunk_id": 5,
        "content": "Learning with Neural Networks', (2014). Figure 1-3. An encoder-decoder architecture with a pair of RNNs (in general, there are many more recurrent layers than those shown here) ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000002_23521f3d35422fa73585da0eee8da7a826bc7f4d395547c3f1947cee7c69db61.png) *Image Description:* The image illustrates an encoder-decoder architecture utilizing recurrent neural networks (RNNs). The encoder block, represented by the blue section, processes a sequence of input tokens (e.g., \"Transformers\", \"are\", \"great\", \"!\",) using a series of RNN cells. The state generated by the encoder is then passed to the decoder block, represented by the red section, which consists of another set of RNN cells. The decoder uses this state to generate a sequence of output tokens (e.g., \"Transformer\", \" sind\", \"grossartig\", \"!\"). This architecture is designed to enable the decoder to generate a coherent output sequence based on the encoded input, effectively translating or summarizing the input sequence. However, it is noted that this architecture may struggle with long sequences due to the information bottleneck caused by compressing the entire input sequence into a single final hidden state, which can lead to the loss of important early information. Although elegant in its simplicity, one weakness of this architecture is that the final hidden state of the encoder creates an information bottleneck : it has to represent the meaning of the whole input sequence because this is all the decoder has access to when generating the output. This is especially challenging for long sequences, where information at the start of the sequence might be lost in the process of compressing everything to a single, fixed representation. Fortunately, there is a way out of this bottleneck by allowing the decoder to have access to all of the encoder's hidden states. The general mechanism for this is called attention , 6 and it is a key component in many modern neural network architectures. Understanding how attention was developed for RNNs will put us in good shape to understand one of the main building blocks of the Transformer architecture. Let's take a deeper look.",
        "metadata": {
            "source": "part1.md",
            "section": "The Encoder-Decoder Framework",
            "position": 5
        }
    },
    {
        "chunk_id": 6,
        "content": "## Attention Mechanisms\nThe main idea behind attention is that instead of producing a single hidden state for the input sequence, the encoder outputs a hidden state at each step that the decoder can access. However, using all the states at the same time would create a huge input for the decoder, so some mechanism is needed to prioritize which states to use. This is where attention comes in: it lets the decoder assign a different amount of weight, or 'attention, ' to each of the encoder states at every decoding timestep. This process is illustrated in Figure 1-4, where the role of attention is shown for predicting the third token in the output sequence. 6 D. Bahdanau, K. Cho, and Y. Bengio, 'Neural Machine Translation by Jointly Learning to Align and Translate', (2014). 4 | Chapter 1: Hello Transformers Figure 1-4. An encoder-decoder architecture with an attention mechanism for a pair of RNNs",
        "metadata": {
            "source": "part1.md",
            "section": "Attention Mechanisms",
            "position": 6
        }
    },
    {
        "chunk_id": 7,
        "content": "## Attention Mechanisms\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000003_d5d2c43f371f725d14cc30703ac6eebd2efabe9e406c6a9b4fd6847945461ddf.png) *Image Description:* The image illustrates an encoder-decoder architecture with an attention mechanism, designed for machine translation. In this architecture, the encoder block on the left represents the input language, which in this example is English. It processes the input sequence \"Transformers are great!\" through a series of RNN cells, generating four states (State 1 to State 4). The decoder block on the right is responsible for generating the output language, which is French in this case. It also consists of RNN cells, but it utilizes an attention mechanism to focus on the most relevant parts of the input sequence at each decoding step. The attention mechanism allows the decoder to align words from the source language to the target language more effectively. For instance, in Figure 1-5, the attention weights are visualized as a heat map, with higher pixel values indicating greater relevance. This visualization demonstrates how the decoder is able to correctly align the English words \"zone\" and \"Area\" with the French words \"Zone\" and \"Area,\" even though they are in different orders in the two languages. The arrows in the figure highlight the alignment between the input and output words, showcasing the decoder's capability to re-order and align words appropriately during the translation process. By focusing on which input tokens are most relevant at each timestep, these attention-based models are able to learn nontrivial alignments between the words in a generated translation and those in a source sentence. For example, Figure 1-5 visualizes the attention weights for an English to French translation model, where each pixel denotes a weight. The figure shows how the decoder is able to correctly align the words 'zone' and ' Area' , which are ordered differently in the two languages. Figure 1-5. RNN encoder-decoder alignment of words in English and the generated translation in French (courtesy of Dzmitry Bahdanau) ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000004_fc0bf63f5c4e70401112a63c8f770e6666abc82f9d3d091709045f30f0fcb5bd.png) *Image Description:* The image illustrates the alignment of words in an English sentence and their corresponding French translation using a Recurrent Neural Network (RNN) encoder-decoder architecture. The alignment is visualized as a diagonal line of squares, where each square represents a word pair from the English sentence on the top and the French translation on the bottom. The squares are shaded from white to black, indicating the degree of alignment and matching between words. The English sentence reads, \"The agreement on the European Economic Area was signed in August 1992.\" The French",
        "metadata": {
            "source": "part1.md",
            "section": "Attention Mechanisms",
            "position": 7
        }
    },
    {
        "chunk_id": 8,
        "content": "French translation on the bottom. The squares are shaded from white to black, indicating the degree of alignment and matching between words. The English sentence reads, \"The agreement on the European Economic Area was signed in August 1992.\" The French translation on the bottom reads, \"L'accord sur la zone économique européenne a été signé en août 1992.\" The alignment shows that words from both sentences are sequentially matched, with the diagonal line highlighting the correct word-to-word correspondence between the two languages. The \"<end>\" markers at the end of each line indicate the end of the sentences. This visualization demonstrates how an RNN can be used to align sentences and improve the quality of translation by showing the alignment path between the source and target languages. | Although attention enabled the production of much better translations, there was still a major shortcoming with using recurrent models for the encoder and decoder: the computations are inherently sequential and cannot be parallelized across the input sequence. With the transformer, a new modeling paradigm was introduced: dispense with recurrence altogether, and instead rely entirely on a special form of attention called self-attention . We'll cover self-attention in more detail in Chapter 3, but the basic idea is to allow attention to operate on all the states in the same layer of the neural network. This is shown in Figure 1-6, where both the encoder and the decoder have their own self-attention mechanisms, whose outputs are fed to feed-forward neural networks (FF NNs). This architecture can be trained much faster than recurrent models and paved the way for many of the recent breakthroughs in NLP . Figure 1-6. Encoder-decoder architecture of the original Transformer ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000005_4f3342769d30c15720cd7aad0c206ecbb3b60683baa175637404828cc0c01b29.png) *Image Description:* The image depicts the encoder-decoder architecture of the original Transformer model, which is a key component in the field of neural machine translation. In this architecture, the encoder block, represented by the blue section, processes the input sequence (e.g., \"Transformers are great!\") by using a series of transformer layers with self-attention mechanisms. Each transformer layer outputs a new state (State 1, State 2, State 3, State 4), which are then passed through feed-forward neural networks (FF NN). The output of the encoder is a set of contextualized representations that capture the meaning of the input sentence. The decoder block, represented by the red section, takes these contextualized representations as input and uses them to generate the target language",
        "metadata": {
            "source": "part1.md",
            "section": "Attention Mechanisms",
            "position": 8
        }
    },
    {
        "chunk_id": 9,
        "content": "output of the encoder is a set of contextualized representations that capture the meaning of the input sentence. The decoder block, represented by the red section, takes these contextualized representations as input and uses them to generate the target language sentence. The decoder also consists of transformer layers with self-attention mechanisms that allow it to consider the entire input sequence while generating the output. The generated states in the decoder are again passed through feed-forward neural networks to produce the final translated sentence in the target language (e.g., \"sind großartig!\"). This entire process is designed to enable the model to learn the translation between the source and target languages effectively, even when trained on a relatively small dataset. In the original Transformer paper, the translation model was trained from scratch on a large corpus of sentence pairs in various languages. However, in many practical applications of NLP we do not have access to large amounts of labeled text data to train our models on. A final piece was missing to get the transformer revolution started: transfer learning.",
        "metadata": {
            "source": "part1.md",
            "section": "Attention Mechanisms",
            "position": 9
        }
    },
    {
        "chunk_id": 10,
        "content": "## Transfer Learning in NLP\nIt is nowadays common practice in computer vision to use transfer learning to train a convolutional neural network like ResNet on one task, and then adapt it to or finetune it on a new task. This allows the network to make use of the knowledge learned from the original task. Architecturally, this involves splitting the model into of a body and a head , where the head is a task-specific network. During training, the weights of the body learn broad features of the source domain, and these weights are used to initialize a new model for the new task. 7 Compared to traditional supervised learning, this approach typically produces high-quality models that can be trained much more 7 Weights are the learnable parameters of a neural network. 6 | Chapter 1: Hello Transformers efficiently on a variety of downstream tasks, and with much less labeled data. A comparison of the two approaches is shown in Figure 1-7. Figure 1-7. Comparison of traditional supervised learning (left) and transfer learning (right)",
        "metadata": {
            "source": "part1.md",
            "section": "Transfer Learning in NLP",
            "position": 10
        }
    },
    {
        "chunk_id": 11,
        "content": "## Transfer Learning in NLP\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000006_7e3ad77ea1484f62b8d68533047b7145e7086db1732b44665758f38391a9a166.png) *Image Description:* The image illustrates a comparison between traditional supervised learning and transfer learning in the context of computer vision. On the left, supervised learning is depicted with two models, A and B, being trained independently on different tasks within the same domain. On the right, transfer learning is shown where knowledge learned by a pre-trained model (Body A) from one domain (Domain A) is transferred and applied to a different task within another domain (Domain B). The pre-trained model (Body A) is initialized with the knowledge from the source task and then fine-tuned with a new task-specific head (Head B) to make predictions (Predictions B). This approach leverages pre-trained models to improve the performance of new models on related tasks with significantly fewer labeled examples compared to training from scratch. In computer vision, the models are first trained on large-scale datasets such as ImageNet, which contain millions of images. This process is called pretraining and its main purpose is to teach the models the basic features of images, such as edges or colors. These pretrained models can then be fine-tuned on a downstream task such as classifying flower species with a relatively small number of labeled examples (usually a few hundred per class). Fine-tuned models typically achieve a higher accuracy than supervised models trained from scratch on the same amount of labeled data. Although transfer learning became the standard approach in computer vision, for many years it was not clear what the analogous pretraining process was for NLP . As a result, NLP applications typically required large amounts of labeled data to achieve high performance. And even then, that performance did not compare to what was achieved in the vision domain. In 2017 and 2018, several research groups proposed new approaches that finally made transfer learning work for NLP. It started with an insight from researchers at OpenAI who obtained strong performance on a sentiment classification task by using features extracted from unsupervised pretraining. 8 This was followed by ULMFiT, which introduced a general framework to adapt pretrained LSTM models for various tasks. 9 As illustrated in Figure 1-8, ULMFiT involves three main steps:",
        "metadata": {
            "source": "part1.md",
            "section": "Transfer Learning in NLP",
            "position": 11
        }
    },
    {
        "chunk_id": 12,
        "content": "## Pretraining\nThe initial training objective is quite simple: predict the next word based on the previous words. This task is referred to as language modeling . The elegance of this approach lies in the fact that no labeled data is required, and one can make use of abundantly available text from sources such as Wikipedia. 10",
        "metadata": {
            "source": "part1.md",
            "section": "Pretraining",
            "position": 12
        }
    },
    {
        "chunk_id": 13,
        "content": "## Domain adaptation\nOnce the language model is pretrained on a large-scale corpus, the next step is to adapt it to the in-domain corpus (e.g., from Wikipedia to the IMDb corpus of movie reviews, as in Figure 1-8). This stage still uses language modeling, but now the model has to predict the next word in the target corpus.",
        "metadata": {
            "source": "part1.md",
            "section": "Domain adaptation",
            "position": 13
        }
    },
    {
        "chunk_id": 14,
        "content": "## Fine-tuning\nIn this step, the language model is fine-tuned with a classification layer for the target task (e.g., classifying the sentiment of movie reviews in Figure 1-8). Figure 1-8. The ULMFiT process (courtesy of Jeremy Howard)",
        "metadata": {
            "source": "part1.md",
            "section": "Fine-tuning",
            "position": 14
        }
    },
    {
        "chunk_id": 15,
        "content": "## Fine-tuning\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000007_886473efe049af6b410f6e6b4919f1fd42a8e12e8738e0621352fecfb53b4931.png) *Image Description:* The ULMFiT process, as depicted in Figure 1-8, illustrates a streamlined approach to pretraining and transfer learning in Natural Language Processing (NLP). The process starts with a language model trained on the Wikitext-103 dataset, which serves as the initial pretraining stage. This pretraining phase is crucial as it allows the model to learn general linguistic patterns and representations. Following the pretraining phase, the model is fine-tuned on the IMDb dataset, which is specifically designed for text classification tasks. This fine-tuning step involves adapting the model to the specific task of classifying movie reviews as positive or negative. The final step in the ULMFiT process involves using the fine-tuned model as a classifier, ready to make predictions on new, unseen data. This entire framework is highlighted as a significant contribution by Jeremy Howard, providing a practical and efficient method for leveraging pretraining and transfer learning in NLP, which was instrumental in advancing the use of transformers in various applications. By introducing a viable framework for pretraining and transfer learning in NLP , ULMFiT provided the missing piece to make transformers take off. In 2018, two transformers were released that combined self-attention with transfer learning: 8 A. Radford, R. Jozefowicz, and I. Sutskever, 'Learning to Generate Reviews and Discovering Sentiment', (2017). 9 A related work at this time was ELMo (Embeddings from Language Models), which showed how pretraining LSTMs could produce high-quality word embeddings for downstream tasks. 10 This is more true for English than for most of the world's languages, where obtaining a large corpus of digitized text can be difficult. Finding ways to bridge this gap is an active area of NLP research and activism. GPT Uses only the decoder part of the Transformer architecture, and the same language modeling approach as ULMFiT. GPT was pretrained on the BookCorpus, 11 which consists of 7,000 unpublished books from a variety of genres including Adventure, Fantasy, and Romance.",
        "metadata": {
            "source": "part1.md",
            "section": "Fine-tuning",
            "position": 15
        }
    },
    {
        "chunk_id": 16,
        "content": "## BERT\nUses the encoder part of the Transformer architecture, and a special form of language modeling called masked language modeling . The objective of masked language modeling is to predict randomly masked words in a text. For example, given a sentence like 'I looked at my [MASK] and saw that [MASK] was late. ' the model needs to predict the most likely candidates for the masked words that are denoted by [MASK] . BERT was pretrained on the BookCorpus and English Wikipedia. GPT and BERT set a new state of the art across a variety of NLP benchmarks and ushered in the age of transformers. However, with different research labs releasing their models in incompatible frameworks (PyTorch or TensorFlow), it wasn't always easy for NLP practitioners to port these models to their own applications. With the release of Transformers, a unified API across more than 50 architectures was progressively built. This library catalyzed the explosion of research into transformers and quickly trickled down to NLP practitioners, making it easy to integrate these models into many real-life applications today. Let's have a look!",
        "metadata": {
            "source": "part1.md",
            "section": "BERT",
            "position": 16
        }
    },
    {
        "chunk_id": 17,
        "content": "## Hugging Face Transformers: Bridging the Gap\nApplying a novel machine learning architecture to a new task can be a complex undertaking, and usually involves the following steps: 1. Implement the model architecture in code, typically based on PyTorch or TensorFlow. 2. Load the pretrained weights (if available) from a server. 3. Preprocess the inputs, pass them through the model, and apply some taskspecific postprocessing. 4. Implement dataloaders and define loss functions and optimizers to train the model. 11 Y. Zhu et al., ' Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books', (2015). Each of these steps requires custom logic for each model and task. Traditionally (but not always!), when research groups publish a new article, they will also release the code along with the model weights. However, this code is rarely standardized and often requires days of engineering to adapt to new use cases. This is where Transformers comes to the NLP practitioner's rescue! It provides a standardized interface to a wide range of transformer models as well as code and tools to adapt these models to new use cases. The library currently supports three major deep learning frameworks (PyTorch, TensorFlow, and JAX) and allows you to easily switch between them. In addition, it provides task-specific heads so you can easily fine-tune transformers on downstream tasks such as text classification, named entity recognition, and question answering. This reduces the time it takes a practitioner to train and test a handful of models from a week to a single afternoon! You'll see this for yourself in the next section, where we show that with just a few lines of code, Transformers can be applied to tackle some of the most common NLP applications that you're likely to encounter in the wild.",
        "metadata": {
            "source": "part1.md",
            "section": "Hugging Face Transformers: Bridging the Gap",
            "position": 17
        }
    },
    {
        "chunk_id": 18,
        "content": "## A Tour of Transformer Applications\nEvery NLP task starts with a piece of text, like the following made-up customer feedback about a certain online order: text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\" Depending on your application, the text you're working with could be a legal contract, a product description, or something else entirely. In the case of customer feedback, you would probably like to know whether the feedback is positive or negative. This task is called sentiment analysis and is part of the broader topic of text classification that we'll explore in Chapter 2. For now, let's have a look at what it takes to extract the sentiment from our piece of text using Transformers.",
        "metadata": {
            "source": "part1.md",
            "section": "A Tour of Transformer Applications",
            "position": 18
        }
    },
    {
        "chunk_id": 19,
        "content": "## Text Classification\nAs we'll see in later chapters, Transformers has a layered API that allows you to interact with the library at various levels of abstraction. In this chapter we'll start with pipelines , which abstract away all the steps needed to convert raw text into a set of predictions from a fine-tuned model.",
        "metadata": {
            "source": "part1.md",
            "section": "Text Classification",
            "position": 19
        }
    },
    {
        "chunk_id": 20,
        "content": "## Text Classification\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000008_7ab232049b93de54191c16abd7bef1f8055fad39d7c8b2397b02f8ae9566ba93.png) *Image Description:* [Image description failed] In Transformers, we instantiate a pipeline by calling the pipeline() function and providing the name of the task we are interested in: ``` from transformers import pipeline classifier = pipeline(\"text-classification\") ``` The first time you run this code you'll see a few progress bars appear because the pipeline automatically downloads the model weights from the Hugging Face Hub. The second time you instantiate the pipeline, the library will notice that you've already downloaded the weights and will use the cached version instead. By default, the text-classification pipeline uses a model that's designed for sentiment analysis, but it also supports multiclass and multilabel classification. Now that we have our pipeline, let's generate some predictions! Each pipeline takes a string of text (or a list of strings) as input and returns a list of predictions. Each prediction is a Python dictionary, so we can use Pandas to display them nicely as a DataFrame : ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000009_6e88fadc1f71b313be8bb814482f1bd5ebc806eecbb26e78114a80e78df40035.png) *Image Description:* The image shows a Python script snippet using Pandas to display the output of a sentiment analysis pipeline. The script imports the Pandas library and then calls a classifier on some text, storing the results in the `outputs` variable. These outputs are then converted into a DataFrame using `pd.DataFrame(outputs)`, which is displayed with two columns: `label` and `score`. The example output indicates that the classifier has predicted a negative sentiment with a high confidence score of 0.901546. This suggests that the text is likely from an angry customer's complaint, aligning with the context provided in the question. In this case the model is very confident that the text has a negative sentiment, which makes sense given that we're dealing with a complaint from an angry customer! Note that for sentiment analysis tasks the pipeline only returns one of the POSITIVE or NEG ATIVE labels, since the other can be inferred by computing 1-score . Let's now take a look at another common task, identifying named entities in text.",
        "metadata": {
            "source": "part1.md",
            "section": "Text Classification",
            "position": 20
        }
    },
    {
        "chunk_id": 21,
        "content": "## Named Entity Recognition\nPredicting the sentiment of customer feedback is a good first step, but you often want to know if the feedback was about a particular item or service. In NLP , real-world objects like products, places, and people are called named entities , and extracting them from text is called named entity recognition (NER). We can apply NER by loading the corresponding pipeline and feeding our customer review to it: ``` ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\") outputs = ner_tagger(text) pd.DataFrame(outputs) ```",
        "metadata": {
            "source": "part1.md",
            "section": "Named Entity Recognition",
            "position": 21
        }
    },
    {
        "chunk_id": 22,
        "table": {
            "headers": [
                "",
                "entity_group",
                "score",
                "word",
                "start",
                "end"
            ],
            "rows": [
                [
                    "0",
                    "ORG",
                    "0.87901",
                    "Amazon",
                    "5",
                    "11"
                ],
                [
                    "1",
                    "MISC",
                    "0.990859",
                    "Optimus Prime",
                    "36",
                    "49"
                ]
            ]
        },
        "metadata": {
            "source": "part1.md",
            "section": "Named Entity Recognition",
            "table_title": "```",
            "position": 22
        }
    },
    {
        "chunk_id": 23,
        "content": "## Named Entity Recognition\n|",
        "metadata": {
            "source": "part1.md",
            "section": "Named Entity Recognition",
            "position": 23
        }
    },
    {
        "chunk_id": 24,
        "table": {
            "headers": [
                "",
                "entity_group",
                "score",
                "word",
                "start",
                "end"
            ],
            "rows": [
                [
                    "2",
                    "LOC",
                    "0.999755",
                    "Germany",
                    "90",
                    "97"
                ],
                [
                    "3",
                    "MISC",
                    "0.556569",
                    "Mega",
                    "208",
                    "212"
                ],
                [
                    "4",
                    "PER",
                    "0.590256",
                    "##tron",
                    "212",
                    "216"
                ],
                [
                    "5",
                    "ORG",
                    "0.669692",
                    "Decept",
                    "253",
                    "259"
                ],
                [
                    "6",
                    "MISC",
                    "0.49835",
                    "##icons",
                    "259",
                    "264"
                ],
                [
                    "7",
                    "MISC",
                    "0.775361",
                    "Megatron",
                    "350",
                    "358"
                ],
                [
                    "8",
                    "MISC",
                    "0.987854",
                    "Optimus Prime",
                    "367",
                    "380"
                ],
                [
                    "9",
                    "PER",
                    "0.812096",
                    "Bumblebee",
                    "502",
                    "511"
                ]
            ]
        },
        "metadata": {
            "source": "part1.md",
            "section": "Named Entity Recognition",
            "table_title": "|",
            "position": 24
        }
    },
    {
        "chunk_id": 25,
        "content": "## Named Entity Recognition\nYou can see that the pipeline detected all the entities and also assigned a category such as ORG (organization), LOC (location), or PER (person) to each of them. Here we used the aggregation\\_strategy argument to group the words according to the model's predictions. For example, the entity 'Optimus Prime' is composed of two words, but is assigned a single category: MISC (miscellaneous). The scores tell us how confident the model was about the entities it identified. We can see that it was least confident about 'Decepticons' and the first occurrence of 'Megatron' , both of which it failed to group as a single entity.",
        "metadata": {
            "source": "part1.md",
            "section": "Named Entity Recognition",
            "position": 25
        }
    },
    {
        "chunk_id": 26,
        "content": "## Named Entity Recognition\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000010_7935bcb252a9dd3caa058842a58555e490ff9ec7d4250362e4c29707fa798df8.png) *Image Description:* The image features a stylized graphic of a blue raven perched on a branch. The raven is depicted with a simple, blocky design, giving it a clean and modern look. The bird is facing left, with its wings slightly spread and its tail feathers fanned out. The branch beneath the raven is a solid, dark blue line that extends diagonally from the bottom left to the top right of the square frame of the image. The background is a plain white, which contrasts with the dark blue of the raven and the branch, making the subject stand out prominently. The overall design is minimalistic, with no additional decorative elements, focusing solely on the raven and the branch. See those weird hash symbols ( # ) in the word column in the previous table? These are produced by the model's tokenizer , which splits words into atomic units called tokens . Y ou'll learn all about tokenization in Chapter 2. Extracting all the named entities in a text is nice, but sometimes we would like to ask more targeted questions. This is where we can use question answering .",
        "metadata": {
            "source": "part1.md",
            "section": "Named Entity Recognition",
            "position": 26
        }
    },
    {
        "chunk_id": 27,
        "content": "## Question Answering\nIn question answering, we provide the model with a passage of text called the context , along with a question whose answer we'd like to extract. The model then returns the span of text corresponding to the answer. Let's see what we get when we ask a specific question about our customer feedback: ``` reader = pipeline(\"question-answering\") question = \"What does the customer want?\" outputs = reader(question=question, context=text) pd.DataFrame([outputs]) ```",
        "metadata": {
            "source": "part1.md",
            "section": "Question Answering",
            "position": 27
        }
    },
    {
        "chunk_id": 28,
        "table": {
            "headers": [
                "",
                "score",
                "start",
                "end",
                "answer"
            ],
            "rows": [
                [
                    "0",
                    "0.631291",
                    "335",
                    "358",
                    "an exchange of Megatron"
                ]
            ]
        },
        "metadata": {
            "source": "part1.md",
            "section": "Question Answering",
            "table_title": "```",
            "position": 28
        }
    },
    {
        "chunk_id": 29,
        "content": "## Question Answering\nWe can see that along with the answer, the pipeline also returned start and end integers that correspond to the character indices where the answer span was found (just like with NER tagging). There are several flavors of question answering that we will investigate in Chapter 7, but this particular kind is called extractive question answering because the answer is extracted directly from the text. With this approach you can read and extract relevant information quickly from a customer's feedback. But what if you get a mountain of long-winded complaints and you don't have the time to read them all? Let's see if a summarization model can help!",
        "metadata": {
            "source": "part1.md",
            "section": "Question Answering",
            "position": 29
        }
    },
    {
        "chunk_id": 30,
        "content": "## Summarization\nThe goal of text summarization is to take a long text as input and generate a short version with all the relevant facts. This is a much more complicated task than the previous ones since it requires the model to generate coherent text. In what should be a familiar pattern by now, we can instantiate a summarization pipeline as follows: ``` summarizer = pipeline(\"summarization\") outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True) print (outputs[0]['summary_text']) Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. ``` This summary isn't too bad! Although parts of the original text have been copied, the model was able to capture the essence of the problem and correctly identify that 'Bumblebee' (which appeared at the end) was the author of the complaint. In this example you can also see that we passed some keyword arguments like max\\_length and clean\\_up\\_tokenization\\_spaces to the pipeline; these allow us to tweak the outputs at runtime. But what happens when you get feedback that is in a language you don't understand? You could use Google Translate, or you can use your very own transformer to translate it for you!",
        "metadata": {
            "source": "part1.md",
            "section": "Summarization",
            "position": 30
        }
    },
    {
        "chunk_id": 31,
        "content": "## Translation\nLike summarization, translation is a task where the output consists of generated text. Let's use a translation pipeline to translate an English text to German: ``` translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\") print (outputs[0]['translation_text']) ``` ``` outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100) Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus ``` Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket öffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von | Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie können mein Dilemma verstehen. Um das Problem zu lösen, Ich fordere einen Austausch von Megatron für die Optimus Prime Figur habe ich bestellt. Anbei sind Kopien meiner Aufzeichnungen über diesen Kauf. Ich erwarte, bald von Ihnen zu hören. Aufrichtig, Bumblebee. Again, the model produced a very good translation that correctly uses German's formal pronouns, like 'Ihrem' and 'Sie.' Here we've also shown how you can override the default model in the pipeline to pick the best one for your application-and you can find models for thousands of language pairs on the Hugging Face Hub. Before we take a step back and look at the whole Hugging Face ecosystem, let's examine one last application.",
        "metadata": {
            "source": "part1.md",
            "section": "Translation",
            "position": 31
        }
    },
    {
        "chunk_id": 32,
        "content": "## Text Generation\nLet's say you would like to be able to provide faster replies to customer feedback by having access to an autocomplete function. With a text generation model you can do this as follows: ``` generator = pipeline(\"text-generation\") response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\" prompt = text + \" \\n\\n Customer service response: \\n \" + response outputs = generator(prompt, max_length=200) print (outputs[0]['generated_text']) Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee. Customer service response: Dear Bumblebee, I am sorry to hear that your order was mixed up. The order was completely mislabeled, which is very common in our online store, but I can appreciate it because it was my understanding from this site and our customer service of the previous day that your order was not made correct in our mind and that we are in a process of resolving this matter. We can assure you that your order ``` OK, maybe we wouldn't want to use this completion to calm Bumblebee down, but you get the general idea. Now that you've seen a few cool applications of transformer models, you might be wondering where the training happens. All of the models that we've used in this chapter are publicly available and already fine-tuned for the task at hand. In general, however, you'll want to fine-tune models on your own data, and in the following chapters you will learn how to do just that. But training a model is just a small piece of any NLP project-being able to efficiently process data, share results with colleagues, and make your work reproducible are key components too. Fortunately, Transformers is surrounded by a big ecosystem of useful tools that support much of the modern machine learning workflow. Let's take a look.",
        "metadata": {
            "source": "part1.md",
            "section": "Text Generation",
            "position": 32
        }
    },
    {
        "chunk_id": 33,
        "content": "ecosystem of useful tools that support much of the modern machine learning workflow. Let's take a look.",
        "metadata": {
            "source": "part1.md",
            "section": "Text Generation",
            "position": 33
        }
    },
    {
        "chunk_id": 34,
        "content": "## The Hugging Face Ecosystem\nWhat started with Transformers has quickly grown into a whole ecosystem consisting of many libraries and tools to accelerate your NLP and machine learning projects. The Hugging Face ecosystem consists of mainly two parts: a family of libraries and the Hub, as shown in Figure 1-9. The libraries provide the code while the Hub provides the pretrained model weights, datasets, scripts for the evaluation metrics, and more. In this section we'll have a brief look at the various components. We'll skip Transformers, as we've already discussed it and we will see a lot more of it throughout the course of the book. Figure 1-9. An overview of the Hugging Face ecosystem",
        "metadata": {
            "source": "part1.md",
            "section": "The Hugging Face Ecosystem",
            "position": 34
        }
    },
    {
        "chunk_id": 35,
        "content": "## The Hugging Face Ecosystem\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000011_2af44270a22fe028ab75354a2f191a58aeb918cd800ac4fd7f86f1f385d10938.png) *Image Description:* The image illustrates the structure of the Hugging Face ecosystem, focusing on the Hugging Face Hub. At the top, the Hub is prominently displayed with a smiling emoji, indicating a friendly and accessible platform. Below the Hub, four key components are listed: Models, Datasets, Metrics, and Docs, each represented by a separate box. These components are interconnected, suggesting that they are integral parts of the ecosystem and often used together. The lower section of the diagram is labeled \"Tokenizers,\" \"Transformers,\" and \"Datasets,\" which are essential tools within the ecosystem for processing and working with text data. These components are further connected to \"Accelerate,\" indicating that this tool is crucial for the optimization and performance enhancement of the models and datasets used within the Hub. The arrows between the components suggest a flow of interaction and utility, emphasizing that the ecosystem is designed to be a cohesive and interconnected system for managing and utilizing various resources in natural language processing and machine learning tasks.",
        "metadata": {
            "source": "part1.md",
            "section": "The Hugging Face Ecosystem",
            "position": 35
        }
    },
    {
        "chunk_id": 36,
        "content": "## The Hugging Face Hub\nAs outlined earlier, transfer learning is one of the key factors driving the success of transformers because it makes it possible to reuse pretrained models for new tasks. Consequently, it is crucial to be able to load pretrained models quickly and run experiments with them. The Hugging Face Hub hosts over 20,000 freely available models. As shown in Figure 1-10, there are filters for tasks, frameworks, datasets, and more that are designed to help you navigate the Hub and quickly find promising candidates. As we've seen with the pipelines, loading a promising model in your code is then literally just one line of code away. This makes experimenting with a wide range of models simple, and allows you to focus on the domain-specific parts of your project. Figure 1-10. The Models page of the Hugging Face Hub, showing filters on the left and a list of models on the right",
        "metadata": {
            "source": "part1.md",
            "section": "The Hugging Face Hub",
            "position": 36
        }
    },
    {
        "chunk_id": 37,
        "content": "## The Hugging Face Hub\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000012_993ea654b32e62ee953f01376dc820a5fd5809860b6ea51690f4bc03e720408d.png) *Image Description:* The image shows the Models page of the Hugging Face Hub, a platform for accessing pre-trained machine learning models, datasets, and scripts. On the left side, there are navigation links for different tasks such as Fill-Mask, Question Answering, Text Classification, Text2Text Generation, Translation, Sentence Similarity, and Libraries like PyTorch, TensorFlow, and JAX. Below these links, datasets such as Wikipedia, Common Voice, BookCorpus, DCEP, EuroParl, JRC-Acquis, GLUE, and SQUAD are listed. The central part of the image displays a list of models, with over 25,493 available, sorted by the most downloaded. Some of the models listed include bert-base-uncased, xlm-roberta-base, roberta-large, distilbert-base-uncased, and gpt2. Each model entry provides information such as the task it is used for, the latest update date, the number of downloads, and the number of stars (feedback or popularity). The top of the page includes a search bar for finding models, datasets, and users, along with navigation links for Models, Datasets, Resources, Solutions, and Pricing. In addition to model weights, the Hub also hosts datasets and scripts for computing metrics, which let you reproduce published results or leverage additional data for your application. The Hub also provides model and dataset cards to document the contents of models and datasets and help you make an informed decision about whether they're the right ones for you. One of the coolest features of the Hub is that you can try out any model directly through the various task-specific interactive widgets as shown in Figure 1-11. ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000013_d4712d76a559b9b13347365ec331a4b72c53cf0f0674fad3797f1c83e72c50f4.png) *Image Description:* The image is a screenshot from the Hugging Face Hub, showcasing a model card for the \"BERT base model (uncased).\" The left side of the image provides detailed information about the model, including its type (a pretrained model on the English language using a masked language modeling (MLM) objective), its history (introduced in a specific paper and released in a particular repository), and a disclaimer indicating that the model card was written by the Hugging Face team since the original team did not create a card for this model. Below this, there is a section titled \"Model description\" which further elaborates on the nature of the BERT model, describing it as a transformer model pretrained on a large corpus of English data in a self-supervised manner. The right side of the image features an interactive widget labeled \"Inference widget,\" which allows users to interact with the model by inputting text",
        "metadata": {
            "source": "part1.md",
            "section": "The Hugging Face Hub",
            "position": 37
        }
    },
    {
        "chunk_id": 38,
        "content": "as a transformer model pretrained on a large corpus of English data in a self-supervised manner. The right side of the image features an interactive widget labeled \"Inference widget,\" which allows users to interact with the model by inputting text that includes a \"[MASK]\" placeholder. Users can click the \"Compute\" button to run the model on this input and generate a prediction, with the results shown below. The widget also offers options to view the computation time on CPU, select a different model card, and access JSON output for more detailed results. Additionally, there is a section indicating that the model has been downloaded Figure 1-11. An example model card from the Hugging Face Hub: the inference widget, which allows you to interact with the model, is shown on the right ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000014_09a9df5bf110c115185a7810bddb92c455a6c85c85b13d3bb28b1dce018ee00e.png) *Image Description:* [Image description failed] Let's continue our tour with Tokenizers. ![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000015_781972b3b234b4a6c4f81b4cd78663f51b6e7071d0ba67531a8c1e53e35398f3.png) *Image Description:* The image depicts a stylized graphic of a crow, which is a common logo or icon used to represent or symbolize the Hugging Face platform, a popular open-source library for natural language processing in Python. The crow is rendered in a deep blue color and is encased within a square border, giving the graphic a logo-like appearance. PyTorch and TensorFlow also offer hubs of their own and are worth checking out if a particular model or dataset is not available on the Hugging Face Hub.",
        "metadata": {
            "source": "part1.md",
            "section": "The Hugging Face Hub",
            "position": 38
        }
    },
    {
        "chunk_id": 39,
        "content": "## Hugging Face Tokenizers\nBehind each of the pipeline examples that we've seen in this chapter is a tokenization step that splits the raw text into smaller pieces called tokens. We'll see how this works in detail in Chapter 2, but for now it's enough to understand that tokens may be words, parts of words, or just characters like punctuation. Transformer models are trained on numerical representations of these tokens, so getting this step right is pretty important for the whole NLP project! Tokenizers provides many tokenization strategies and is extremely fast at tokenizing text thanks to its Rust backend. 12 It also takes care of all the pre- and postprocessing steps, such as normalizing the inputs and transforming the model outputs to the required format. With Tokenizers, we can load a tokenizer in the same way we can load pretrained model weights with Transformers. 12 Rust is a high-performance programming language. We need a dataset and metrics to train and evaluate models, so let's take a look at Datasets, which is in charge of that aspect.",
        "metadata": {
            "source": "part1.md",
            "section": "Hugging Face Tokenizers",
            "position": 39
        }
    },
    {
        "chunk_id": 40,
        "content": "## Hugging Face Datasets\nLoading, processing, and storing datasets can be a cumbersome process, especially when the datasets get too large to fit in your laptop's RAM. In addition, you usually need to implement various scripts to download the data and transform it into a standard format. Datasets simplifies this process by providing a standard interface for thousands of datasets that can be found on the Hub. It also provides smart caching (so you don't have to redo your preprocessing each time you run your code) and avoids RAM limitations by leveraging a special mechanism called memory mapping that stores the contents of a file in virtual memory and enables multiple processes to modify a file more efficiently. The library is also interoperable with popular frameworks like Pandas and NumPy, so you don't have to leave the comfort of your favorite data wrangling tools. Having a good dataset and powerful model is worthless, however, if you can't reliably measure the performance. Unfortunately, classic NLP metrics come with many different implementations that can vary slightly and lead to deceptive results. By providing the scripts for many metrics, Datasets helps make experiments more reproducible and the results more trustworthy.",
        "metadata": {
            "source": "part1.md",
            "section": "Hugging Face Datasets",
            "position": 40
        }
    },
    {
        "chunk_id": 41,
        "content": "## Hugging Face Datasets\n![Image](E:\\CODE\\RAG\\Documents\\output\\part1_artifacts\\image_000016_ceff42d6696226bc6e2980d43ecc7a840e40b8853f0b605a0d5c540ba51d90a9.png) *Image Description:* [Image description failed] With the Transformers, Tokenizers, and Datasets libraries we have everything we need to train our very own transformer models! However, as we'll see in Chapter 10 there are situations where we need fine-grained control over the training loop. That's where the last library of the ecosystem comes into play: Accelerate.",
        "metadata": {
            "source": "part1.md",
            "section": "Hugging Face Datasets",
            "position": 41
        }
    },
    {
        "chunk_id": 42,
        "content": "## Hugging Face Accelerate\nIf you've ever had to write your own training script in PyTorch, chances are that you've had some headaches when trying to port the code that runs on your laptop to the code that runs on your organization's cluster. Accelerate adds a layer of abstraction to your normal training loops that takes care of all the custom logic necessary for the training infrastructure. This literally accelerates your workflow by simplifying the change of infrastructure when necessary. This sums up the core components of Hugging Face's open source ecosystem. But before wrapping up this chapter, let's take a look at a few of the common challenges that come with trying to deploy transformers in the real world.",
        "metadata": {
            "source": "part1.md",
            "section": "Hugging Face Accelerate",
            "position": 42
        }
    },
    {
        "chunk_id": 43,
        "content": "## Main Challenges with Transformers\nIn this chapter we've gotten a glimpse of the wide range of NLP tasks that can be tackled with transformer models. Reading the media headlines, it can sometimes sound like their capabilities are limitless. However, despite their usefulness, transformers are far from being a silver bullet. Here are a few challenges associated with them that we will explore throughout the book:",
        "metadata": {
            "source": "part1.md",
            "section": "Main Challenges with Transformers",
            "position": 43
        }
    },
    {
        "chunk_id": 44,
        "content": "## Language\nNLP research is dominated by the English language. There are several models for other languages, but it is harder to find pretrained models for rare or lowresource languages. In Chapter 4, we'll explore multilingual transformers and their ability to perform zero-shot cross-lingual transfer.",
        "metadata": {
            "source": "part1.md",
            "section": "Language",
            "position": 44
        }
    },
    {
        "chunk_id": 45,
        "content": "## Data availability\nAlthough we can use transfer learning to dramatically reduce the amount of labeled training data our models need, it is still a lot compared to how much a human needs to perform the task. Tackling scenarios where you have little to no labeled data is the subject of Chapter 9.",
        "metadata": {
            "source": "part1.md",
            "section": "Data availability",
            "position": 45
        }
    },
    {
        "chunk_id": 46,
        "content": "## Working with long documents\nSelf-attention works extremely well on paragraph-long texts, but it becomes very expensive when we move to longer texts like whole documents. Approaches to mitigate this are discussed in Chapter 11.",
        "metadata": {
            "source": "part1.md",
            "section": "Working with long documents",
            "position": 46
        }
    },
    {
        "chunk_id": 47,
        "content": "## Opacity\nAs with other deep learning models, transformers are to a large extent opaque. It is hard or impossible to unravel 'why' a model made a certain prediction. This is an especially hard challenge when these models are deployed to make critical decisions. We'll explore some ways to probe the errors of transformer models in Chapters 2 and 4. Bias Transformer models are predominantly pretrained on text data from the internet. This imprints all the biases that are present in the data into the models. Making sure that these are neither racist, sexist, or worse is a challenging task. We discuss some of these issues in more detail in Chapter 10. Although daunting, many of these challenges can be overcome. As well as in the specific chapters mentioned, we will touch on these topics in almost every chapter ahead.",
        "metadata": {
            "source": "part1.md",
            "section": "Opacity",
            "position": 47
        }
    },
    {
        "chunk_id": 48,
        "content": "## Conclusion\nHopefully, by now you are excited to learn how to start training and integrating these versatile models into your own applications! You've seen in this chapter that with just a few lines of code you can use state-of-the-art models for classification, named entity recognition, question answering, translation, and summarization, but this is really just the 'tip of the iceberg. ' In the following chapters you will learn how to adapt transformers to a wide range of use cases, such as building a text classifier, or a lightweight model for production, or even training a language model from scratch. We'll be taking a hands-on approach, which means that for every concept covered there will be accompanying code that you can run on Google Colab or your own GPU machine. Now that we're armed with the basic concepts behind transformers, it's time to get our hands dirty with our first application: text classification. That's the topic of the next chapter!",
        "metadata": {
            "source": "part1.md",
            "section": "Conclusion",
            "position": 48
        }
    }
]